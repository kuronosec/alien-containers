# This example shows how ApMon can be used to send information about a given job 
# to the MonALISA service
import json
import re
import sys
import time
import threading
import docker_utils
import apmon

class Kumon:

    def __init__(self, configurationPath, socketPath):
        self._configurationPath = configurationPath
        self._socketPath        = socketPath
        self._container         = docker_utils.DockerUtils(socketPath)
        
        # Initialize ApMon specifying that it should not send information about the system.
        # Note that in this case the background monitoring process isn't stopped, in case you may
        # want later to monitor a job.
        self._apm = apmon.ApMon(self._configurationPath)
        self._apm.setLogLevel("INFO")
        self._apm.confCheck = False
        self._apm.enableBgMonitoring(False)
        self._apm.setMaxMsgRate(1000)

    def getData(self, containerName):
        # Get job statistics from the available sources
        stats_obj = self._container.getStats(containerName)
        return stats_obj

    def sendData(self, json_obj, jobId):
        for metric_map in self.getMetrics():
            # you can put as many pairs of parameter_name, 
            # parameter_value as you want
            # but be careful not to create packets longer than 8K.
            self._apm.sendParameters("ALICE::UF::KUBERNETES_Jobs", 
                                     jobId,
                                     { metric_map[0]+"."+metric_map[-1]: 
                                       self.getFromDict(json_obj, metric_map)}
                                    )

    def getFromDict(self, dataDict, mapList):
        return reduce(lambda d, k: d[k], mapList, dataDict)

    # Calling a thread per alien container
    def monitoring_thread(self, containerName, containerId):
        print("Start reading the logs from %s" % containerName)
        # Get the real Job Id from the AliEn JobAgent logs.
        # Wait until it shows the real Job Id to really start sending
        # data
        jobId = None
        while jobId is None:
             log_string = self._container.getLogs(containerId, False)
             jobId      = self._container.getRegexpFromLogs(log_string,
                               '^(.*)Putting in the joblog: (\d+)(.*)$',
                               2)

        # When finally we get the real job ID
        print("Start monitoring in Job: %s" % jobId)
        # Only send data about a job while it is alive.
        # Then kill the thread.
        stream = self.getData(containerName)
        for json in stream:
            self.sendData(json, jobId)
            time.sleep(30)
            if not self.isAlive(containerId):
                break
        print("Finish monitoring %s" % containerName)

    def start_monitoring(self):
        # Start reading the list of event generated by the docker engine.
        # We are interested only in the event involving "alien" containers,
        # when they are starting. This way we can start collecting and sending
        # data.
        events_obj = self._container.getEvents(filters= {'event': 'start', 'image': 'test:alien'})
        for event in events_obj:
            print event
            info = self._container.getInfo(event['id'])
            t    = threading.Thread(target=self.monitoring_thread, args = [info['Name'], event['id']])
            t.start()

    def isAlive(self, containerId):
            info = self._container.getInfo(containerId)
            return info['State']['Running']

    def getMetrics(self):
        metricNames = [
        ["precpu_stats", "system_cpu_usage"],
        ["precpu_stats", "cpu_usage", "total_usage"],
        ["precpu_stats", "cpu_usage", "usage_in_kernelmode"],
        ["precpu_stats", "cpu_usage", "usage_in_usermode"],
        ["networks", "eth0", "rx_dropped"],
        ["networks", "eth0", "rx_packets"],
        ["networks", "eth0", "tx_packets"],
        ["networks", "eth0", "tx_dropped"],
        ["networks", "eth0", "rx_bytes"],
        ["networks", "eth0", "rx_errors"],
        ["networks", "eth0", "tx_bytes"],
        ["networks", "eth0", "tx_errors"],
        #["read"],
        ["memory_stats", "max_usage"],
        ["memory_stats", "usage"],
        ["memory_stats", "stats", "total_pgmajfault"],
        ["memory_stats", "stats", "inactive_file"],
        ["memory_stats", "stats", "pgpgin"],
        ["memory_stats", "stats", "total_writeback"],
        ["memory_stats", "stats", "pgmajfault"],
        ["memory_stats", "stats", "total_active_file"],
        ["memory_stats", "stats", "mapped_file"],
        ["memory_stats", "stats", "total_pgpgout"],
        ["memory_stats", "stats", "total_active_anon"],
        ["memory_stats", "stats", "total_pgfault"],
        ["memory_stats", "stats", "total_rss"],
        ["memory_stats", "stats", "writeback"],
        ["memory_stats", "stats", "total_pgpgin"],
        #["memory_stats", "stats", "hierarchical_memory_limit"],
        ["memory_stats", "stats", "inactive_anon"],
        ["memory_stats", "stats", "total_inactive_file"],
        ["memory_stats", "stats", "active_file"],
        ["memory_stats", "stats", "unevictable"],
        ["memory_stats", "stats", "total_mapped_file"],
        ["memory_stats", "stats", "active_anon"],
        ["memory_stats", "stats", "pgfault"],
        ["memory_stats", "stats", "total_cache"],
        ["memory_stats", "stats", "rss_huge"],
        ["memory_stats", "stats", "total_inactive_anon"],
        ["memory_stats", "stats", "pgpgout"],
        ["memory_stats", "stats", "total_unevictable"],
        ["memory_stats", "stats", "total_rss_huge"],
        ["memory_stats", "stats", "rss"],
        ["memory_stats", "stats", "cache"],
        ["memory_stats", "limit"],
        ["memory_stats", "failcnt"]
        ]
        return metricNames
